import{u as o}from"./vue.-sixQ7xP-DCxRBZOG.js";import{c as a,o as s,n as r}from"./index-DXUBQysH.js";const i={class:"markdown-body"},m="ViralVideos.com",f="2014-09-23T00:00:00.000Z",p="2016-07-11T00:00:00.000Z",v="2015-10-04T00:00:00.000Z",w="2021-07-12T00:00:00.000Z",b="An aggregator and ranking system for viral videos.",g=1.23,y=["FRONTEND","WEBSITE"],T=[],k={__name:"index",setup(n,{expose:t}){return t({frontmatter:{name:"ViralVideos.com",start_date:"2014-09-23T00:00:00.000Z",end_date:"2016-07-11T00:00:00.000Z",release_date:"2015-10-04T00:00:00.000Z",close_date:"2021-07-12T00:00:00.000Z",short_description:"An aggregator and ranking system for viral videos.",carousel_aspect:1.23,categories:["FRONTEND","WEBSITE"],meta:[]}}),o({meta:[]}),(l,e)=>(s(),a("div",i,e[0]||(e[0]=[r("<h1>ViralVideos.com</h1><p>Viral Videos was a website that aggregated and ranked videos from YouTube based on how “viral” they are. I redesigned and rebuilt the entire website as my first project working for Max Games. This project was also the first time I used NodeJS for a component of a larger production system.</p><p>The project consisted of three major components:</p><ol><li>A NodeJS scraper that ran on a cron job schedule to access the YouTube API and collect videos that met a certain threshold of interest. This scraper would then gather metadata about the video, generate assets for the website using whatever was available from YouTube, and store the video in a database for manual review.</li><li>A custom backend for content editors to review the collected videos, extract original content such as thumbnails and background artwork, and publish the videos to the website. The killer feature for our editors was a custom video player that allowed them to quickly review a video and extract the necessary content interactively. Editors could scrub through the video, pause, and take a snapshot of the frame they wanted to use as the thumbnail or background. The trick to accomplishing this was to build a proxy for YouTube video files that was served from our domain. This way CORS would not be an issue and the browser would allow us to access the video data directly.</li><li>A frontend for users to browse the collected videos, filter and search for videos based on various criteria, and vote on videos to influence their ranking. The frontend was the last that I built with my old approach of using jQuery and PHP for everything.</li></ol><p>The stack was very successful, and for a period it generated a decent amount of interest and traffic with this formula. We eventually stopped paying for editors to review content but the scraper continued to find the most popular videos on YouTube. The website ran without content editors for 2 years after I left Max Games, closing completely in 2019 after changes to the YouTube API made it impossible to collect videos in the same way.</p><h2>Key Features</h2><ul><li>Multi-faceted search and filtering system that allows users to drill down on</li><li>Cron-based NodeJS script that interfaced with the YouTube API to collect videos meeting a certain threshold of interest each day</li><li>Custom backend for displaying collected videos for content editors to manually review</li><li>Custom video player for content editors to quickly review a video and extract original content such as thumbnails and background artwork</li></ul>",7)])))}};export{g as carousel_aspect,y as categories,w as close_date,k as default,p as end_date,T as meta,m as name,v as release_date,b as short_description,f as start_date};
